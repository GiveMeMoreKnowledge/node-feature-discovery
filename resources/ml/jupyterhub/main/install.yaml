# https://github.com/bitnami/charts/tree/jupyterhub/9.0.11/bitnami/jupyterhub
---
kind: Namespace
apiVersion: v1
metadata:
  name: nova-jupyterhub
  labels:
    nova-platform.io/trusted-ca-bundle: enabled
    nova-platform.io/cluster-monitoring: "true"
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: latest
spec:
  finalizers:
    - kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nova-jupyterhub-hub
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
automountServiceAccountToken: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nova-jupyterhub-proxy
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
automountServiceAccountToken: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nova-jupyterhub-image-puller
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-image-puller
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
automountServiceAccountToken: false
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nova-jupyterhub-singleuser
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-singleuser
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
automountServiceAccountToken: false
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nova-jupyterhub-hub
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
rules:
  - apiGroups:
      - ""
    resources:
      - "pods"
      - "persistentvolumeclaims"
      - "secrets"
      - "services"
    verbs:
      - "get"
      - "watch"
      - "list"
      - "create"
      - "delete"
  - apiGroups:
      - ""
    resources:
      - "events"
    verbs:
      - "get"
      - "watch"
      - "list"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: nova-jupyterhub-proxy:secrets-webhook-mutation-helper
  namespace: nova-jupyterhub
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - delete
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jupyterhub-vault-agent-auth
  namespace: nova-minio
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
  - kind: ServiceAccount
    name: nova-jupyterhub-hub
    namespace: nova-jupyterhub
  - kind: ServiceAccount
    name: nova-jupyterhub-proxy
    namespace: nova-jupyterhub
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nova-jupyterhub-hub
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
subjects:
  - kind: ServiceAccount
    name: nova-jupyterhub-hub
    namespace: nova-jupyterhub
roleRef:
  kind: Role
  name: nova-jupyterhub-hub
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: nova-jupyterhub-proxy:secrets-webhook-mutation-helper
  namespace: nova-jupyterhub
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nova-jupyterhub-proxy:secrets-webhook-mutation-helper
subjects:
  - kind: ServiceAccount
    name: nova-jupyterhub-proxy
    namespace: nova-jupyterhub
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: jupyterhub-vault-agent-config
  namespace: nova-jupyterhub
data:
  vault-agent-config.hcl: |
    pid_file = "/var/run/secrets/vaultproject.io/pid"
    auto_auth {
        method "kubernetes" {
            mount_path = "auth/nova-kubernetes"
            config = {
                role = "nova-system-jupyterhub"
            }
        }
        sink "file" {
            config = {
                path = "/var/run/secrets/vaultproject.io/token"
            }
        }
    }

    template_config {
      exit_on_retry_failure = true
      static_secret_render_interval = "10m"
    }

    template {
      source      = "/tmp/agent/jupyterhub/jupyterhub_config.ctmpl"
      destination = "/etc/jupyterhub/jupyterhub_config.py"
      error_on_missing_key = true
    }

    template {
      source      = "/tmp/agent/jupyterhub/values.ctmpl"
      destination = "/usr/local/etc/jupyterhub/secret/values.yaml"
      error_on_missing_key = true
    }

    template {
      source      = "/tmp/agent/jupyterhub/hub.config.CryptKeeper.keys.ctmpl"
      destination = "/usr/local/etc/jupyterhub/secret/hub.config.CryptKeeper.keys"
      error_on_missing_key = true
    }

    template {
      source      = "/tmp/agent/jupyterhub/hub.config.JupyterHub.cookie_secret.ctmpl"
      destination = "/usr/local/etc/jupyterhub/secret/hub.config.JupyterHub.cookie_secret"
      error_on_missing_key = true
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-jupyterhub-hub-tmpl
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
data:
  ## Taken from upstream JupyterHub chart
  ## Source: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/tree/4.1.0/jupyterhub/files/hub
  jupyterhub_config.ctmpl: |
    # load the config object (satisfies linters)
    c = get_config()  # noqa

    import glob
    import os
    import re
    import sys

    from jupyterhub.utils import url_path_join
    from kubernetes_asyncio import client
    from tornado.httpclient import AsyncHTTPClient

    # Make sure that modules placed in the same directory as the jupyterhub config are added to the pythonpath
    configuration_directory = os.path.dirname(os.path.realpath(__file__))
    sys.path.insert(0, configuration_directory)

    from z2jh import (
        get_config,
        get_name,
        get_name_env,
        get_secret_value,
        set_config_if_not_none,
    )


    def camelCaseify(s):
        """convert snake_case to camelCase

        For the common case where some_value is set from someValue
        so we don't have to specify the name twice.
        """
        return re.sub(r"_([a-z])", lambda m: m.group(1).upper(), s)

    # OAuth2 config
    c.JupyterHub.authenticator_class = "generic-oauth"

    # OAuth2 application info
    # -----------------------
    {{- with secret "identity/oidc/client/oidc-auth-jupyterhub" }}
    c.GenericOAuthenticator.client_id = "{{ .Data.client_id }}"
    c.GenericOAuthenticator.client_secret = "{{ .Data.client_secret }}"
    {{ end -}}
    # Identity provider info
    # ----------------------
    c.GenericOAuthenticator.authorize_url = "https://nova-oauth.${dnsBaseDomain}/ui/vault/identity/oidc/provider/nova/authorize"
    {{- with secret "identity/oidc/provider/nova" }}
    c.GenericOAuthenticator.token_url = "{{ .Data.issuer }}/token"
    c.GenericOAuthenticator.userdata_url = "{{ .Data.issuer }}/userinfo"
    {{ end -}}
    # What we request about the user
    # ------------------------------
    # scope represents requested information about the user, and since we configure
    # this against an OIDC based identity provider, we should request "openid" at
    # least.
    #
    # In this example we include "email" and "groups" as well, and then declare that
    # we should set the username based on the "email" key in the response, and read
    # group membership from the "groups" key in the response.
    #
    c.GenericOAuthenticator.scope = ["openid", "email", "groups"]
    c.GenericOAuthenticator.username_claim = "email"
    c.GenericOAuthenticator.allow_all = True
    c.GenericOAuthenticator.admin_users = {"kubeadmin@${k8sDefaultDnsZone}"}

    # Configure JupyterHub to use the curl backend for making HTTP requests,
    # rather than the pure-python implementations. The default one starts
    # being too slow to make a large number of requests to the proxy API
    # at the rate required.
    AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")

    c.JupyterHub.spawner_class = "kubespawner.KubeSpawner"
    # Location (absolute filepath) for CA certs of the k8s API server.
    c.KubeSpawner.k8s_api_ssl_ca_cert = "/etc/ssl/certs/ca-certificates.crt"

    # Connect to a proxy running in a different pod. Note that *_SERVICE_*
    # environment variables are set by Kubernetes for Services
    # Adapted by Bitnami to allow other service names
    c.ConfigurableHTTPProxy.api_url = (
        f"http://nova-jupyterhub-proxy-api:{os.environ['PROXY_API_SERVICE_PORT']}"
    )
    c.ConfigurableHTTPProxy.should_start = False

    # Do not shut down user pods when hub is restarted
    c.JupyterHub.cleanup_servers = False

    # Check that the proxy has routes appropriately setup
    c.JupyterHub.last_activity_interval = 60

    # Don't wait at all before redirecting a spawning user to the progress page
    c.JupyterHub.tornado_settings = {
        "slow_spawn_timeout": 0,
    }

    # Allow unauthenticated access to metrics 
    c.JupyterHub.authenticate_prometheus = False


    # configure the hub db connection
    db_type = get_config("hub.db.type")
    if db_type == "sqlite-pvc":
        c.JupyterHub.db_url = "sqlite:////srv/jupyterhub/jupyterhub.sqlite"
    elif db_type == "sqlite-memory":
        c.JupyterHub.db_url = "sqlite://"
    else:
        set_config_if_not_none(c.JupyterHub, "db_url", "hub.db.url")
    db_password = get_secret_value("hub.db.password", None)
    if db_password is not None:
        if db_type == "mysql":
            os.environ["MYSQL_PWD"] = db_password
        elif db_type == "postgres":
            os.environ["PGPASSWORD"] = db_password
        else:
            print(f"Warning: hub.db.password is ignored for hub.db.type={db_type}")


    # c.JupyterHub configuration from Helm chart's configmap
    for trait, cfg_key in (
        ("concurrent_spawn_limit", None),
        ("active_server_limit", None),
        ("base_url", None),
        ("allow_named_servers", None),
        ("named_server_limit_per_user", None),
        ("authenticate_prometheus", None),
        ("redirect_to_server", None),
        ("shutdown_on_logout", None),
        ("template_paths", None),
        ("template_vars", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.JupyterHub, trait, "hub." + cfg_key)

    # hub_bind_url configures what the JupyterHub process within the hub pod's
    # container should listen to.
    hub_container_port = 8081
    c.JupyterHub.hub_bind_url = f"http://:{hub_container_port}"

    # hub_connect_url is the URL for connecting to the hub for use by external
    # JupyterHub services such as the proxy. Note that *_SERVICE_* environment
    # variables are set by Kubernetes for Services.
    c.JupyterHub.hub_connect_url = (
        f"http://nova-jupyterhub-hub:{os.environ['HUB_SERVICE_PORT']}"
    )

    # implement common labels
    # This mimics the jupyterhub.commonLabels helper, but declares managed-by to
    # kubespawner instead of helm.
    #
    # The labels app and release are old labels enabled to be deleted in z2jh 5, but
    # for now retained to avoid a breaking change in z2jh 4 that would force user
    # server restarts. Restarts would be required because NetworkPolicy resources
    # must select old/new pods with labels that then needs to be seen on both
    # old/new pods, and we want these resources to keep functioning for old/new user
    # server pods during an upgrade.
    #
    common_labels = c.KubeSpawner.common_labels = {}
    common_labels["app.kubernetes.io/name"] = common_labels["app"] = get_config(
        "nameOverride",
        default=get_config("Chart.Name", "jupyterhub"),
    )
    release = get_config("Release.Name")
    if release:
        common_labels["app.kubernetes.io/instance"] = common_labels["release"] = release
    chart_name = get_config("Chart.Name")
    chart_version = get_config("Chart.Version")
    if chart_name and chart_version:
        common_labels["helm.sh/chart"] = common_labels["chart"] = (
            f"{chart_name}-{chart_version.replace('+', '_')}"
        )
    common_labels["app.kubernetes.io/managed-by"] = "kubespawner"

    c.KubeSpawner.namespace = os.environ.get("POD_NAMESPACE", "default")

    # Max number of consecutive failures before the Hub restarts itself
    set_config_if_not_none(
        c.Spawner,
        "consecutive_failure_limit",
        "hub.consecutiveFailureLimit",
    )

    for trait, cfg_key in (
        ("pod_name_template", None),
        ("start_timeout", None),
        ("image_pull_policy", "image.pullPolicy"),
        # ('image_pull_secrets', 'image.pullSecrets'), # Managed manually below
        ("events_enabled", "events"),
        ("extra_labels", None),
        ("extra_annotations", None),
        # ("allow_privilege_escalation", None), # Managed manually below
        ("uid", None),
        ("fs_gid", None),
        ("service_account", "serviceAccountName"),
        ("storage_extra_labels", "storage.extraLabels"),
        # ("tolerations", "extraTolerations"), # Managed manually below
        ("node_selector", None),
        ("node_affinity_required", "extraNodeAffinity.required"),
        ("node_affinity_preferred", "extraNodeAffinity.preferred"),
        ("pod_affinity_required", "extraPodAffinity.required"),
        ("pod_affinity_preferred", "extraPodAffinity.preferred"),
        ("pod_anti_affinity_required", "extraPodAntiAffinity.required"),
        ("pod_anti_affinity_preferred", "extraPodAntiAffinity.preferred"),
        ("lifecycle_hooks", None),
        ("init_containers", None),
        ("extra_containers", None),
        ("mem_limit", "memory.limit"),
        ("mem_guarantee", "memory.guarantee"),
        ("cpu_limit", "cpu.limit"),
        ("cpu_guarantee", "cpu.guarantee"),
        ("extra_resource_limits", "extraResource.limits"),
        ("extra_resource_guarantees", "extraResource.guarantees"),
        ("environment", "extraEnv"),
        ("profile_list", None),
        ("extra_pod_config", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.KubeSpawner, trait, "singleuser." + cfg_key)

    image = get_config("singleuser.image.name")
    if image:
        tag = get_config("singleuser.image.tag")
        if tag:
            image = f"{image}:{tag}"

        c.KubeSpawner.image = image

    # allow_privilege_escalation defaults to False in KubeSpawner 2+. Since its a
    # property where None, False, and True all are valid values that users of the
    # Helm chart may want to set, we can't use the set_config_if_not_none helper
    # function as someone may want to override the default False value to None.
    #
    c.KubeSpawner.allow_privilege_escalation = get_config(
        "singleuser.allowPrivilegeEscalation"
    )

    # Combine imagePullSecret.create (single), imagePullSecrets (list), and
    # singleuser.image.pullSecrets (list).
    image_pull_secrets = []
    if get_config("imagePullSecret.automaticReferenceInjection") and get_config(
        "imagePullSecret.create"
    ):
        image_pull_secrets.append(get_name("image-pull-secret"))
    if get_config("imagePullSecrets"):
        image_pull_secrets.extend(get_config("imagePullSecrets"))
    if get_config("singleuser.image.pullSecrets"):
        image_pull_secrets.extend(get_config("singleuser.image.pullSecrets"))
    if image_pull_secrets:
        c.KubeSpawner.image_pull_secrets = image_pull_secrets

    # scheduling:
    if get_config("scheduling.userScheduler.enabled"):
        c.KubeSpawner.scheduler_name = get_name("user-scheduler")
    if get_config("scheduling.podPriority.enabled"):
        c.KubeSpawner.priority_class_name = get_name("priority")

    # add node-purpose affinity
    match_node_purpose = get_config("scheduling.userPods.nodeAffinity.matchNodePurpose")
    if match_node_purpose:
        node_selector = dict(
            matchExpressions=[
                dict(
                    key="hub.jupyter.org/node-purpose",
                    operator="In",
                    values=["user"],
                )
            ],
        )
        if match_node_purpose == "prefer":
            c.KubeSpawner.node_affinity_preferred.append(
                dict(
                    weight=100,
                    preference=node_selector,
                ),
            )
        elif match_node_purpose == "require":
            c.KubeSpawner.node_affinity_required.append(node_selector)
        elif match_node_purpose == "ignore":
            pass
        else:
            raise ValueError(
                f"Unrecognized value for matchNodePurpose: {match_node_purpose}"
            )

    # Combine the common tolerations for user pods with singleuser tolerations
    scheduling_user_pods_tolerations = get_config("scheduling.userPods.tolerations", [])
    singleuser_extra_tolerations = get_config("singleuser.extraTolerations", [])
    tolerations = scheduling_user_pods_tolerations + singleuser_extra_tolerations
    if tolerations:
        c.KubeSpawner.tolerations = tolerations

    # Configure dynamically provisioning pvc
    storage_type = get_config("singleuser.storage.type")
    if storage_type == "dynamic":
        pvc_name_template = get_config("singleuser.storage.dynamic.pvcNameTemplate")
        if pvc_name_template:
            c.KubeSpawner.pvc_name_template = pvc_name_template
        volume_name_template = get_config("singleuser.storage.dynamic.volumeNameTemplate")
        c.KubeSpawner.storage_pvc_ensure = True
        set_config_if_not_none(
            c.KubeSpawner, "storage_class", "singleuser.storage.dynamic.storageClass"
        )
        set_config_if_not_none(
            c.KubeSpawner,
            "storage_access_modes",
            "singleuser.storage.dynamic.storageAccessModes",
        )
        set_config_if_not_none(
            c.KubeSpawner, "storage_capacity", "singleuser.storage.capacity"
        )

        # Add volumes to singleuser pods
        c.KubeSpawner.volumes = [
            {
                "name": volume_name_template,
                "persistentVolumeClaim": {"claimName": "{pvc_name}"},
            }
        ]
        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": volume_name_template,
                "subPath": get_config("singleuser.storage.dynamic.subPath"),
            }
        ]
    elif storage_type == "static":
        pvc_claim_name = get_config("singleuser.storage.static.pvcName")
        c.KubeSpawner.volumes = [
            {"name": "home", "persistentVolumeClaim": {"claimName": pvc_claim_name}}
        ]

        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": "home",
                "subPath": get_config("singleuser.storage.static.subPath"),
            }
        ]

    # Inject singleuser.extraFiles as volumes and volumeMounts with data loaded from
    # the dedicated k8s Secret prepared to hold the extraFiles actual content.
    extra_files = get_config("singleuser.extraFiles", {})
    if extra_files:
        volume = {
            "name": "files",
        }
        items = []
        for file_key, file_details in extra_files.items():
            # Each item is a mapping of a key in the k8s Secret to a path in this
            # abstract volume, the goal is to enable us to set the mode /
            # permissions only though so we don't change the mapping.
            item = {
                "key": file_key,
                "path": file_key,
            }
            if "mode" in file_details:
                item["mode"] = file_details["mode"]
            items.append(item)
        volume["secret"] = {
            "secretName": get_name("singleuser"),
            "items": items,
        }
        c.KubeSpawner.volumes.append(volume)

        volume_mounts = []
        for file_key, file_details in extra_files.items():
            volume_mounts.append(
                {
                    "mountPath": file_details["mountPath"],
                    "subPath": file_key,
                    "name": "files",
                }
            )
        c.KubeSpawner.volume_mounts.extend(volume_mounts)

    # Inject extraVolumes / extraVolumeMounts
    c.KubeSpawner.volumes.extend(get_config("singleuser.storage.extraVolumes", []))
    c.KubeSpawner.volume_mounts.extend(
        get_config("singleuser.storage.extraVolumeMounts", [])
    )

    c.JupyterHub.services = []
    c.JupyterHub.load_roles = []

    # jupyterhub-idle-culler's permissions are scoped to what it needs only, see
    # https://github.com/jupyterhub/jupyterhub-idle-culler#permissions.
    #
    if get_config("cull.enabled", False):
        jupyterhub_idle_culler_role = {
            "name": "jupyterhub-idle-culler",
            "scopes": [
                "list:users",
                "read:users:activity",
                "read:servers",
                "delete:servers",
                # "admin:users", # dynamically added if --cull-users is passed
            ],
            # assign the role to a jupyterhub service, so it gains these permissions
            "services": ["jupyterhub-idle-culler"],
        }

        cull_cmd = ["python3", "-m", "jupyterhub_idle_culler"]
        base_url = c.JupyterHub.get("base_url", "/")
        cull_cmd.append("--url=http://localhost:8081" + url_path_join(base_url, "hub/api"))

        cull_timeout = get_config("cull.timeout")
        if cull_timeout:
            cull_cmd.append(f"--timeout={cull_timeout}")

        cull_every = get_config("cull.every")
        if cull_every:
            cull_cmd.append(f"--cull-every={cull_every}")

        cull_concurrency = get_config("cull.concurrency")
        if cull_concurrency:
            cull_cmd.append(f"--concurrency={cull_concurrency}")

        if get_config("cull.users"):
            cull_cmd.append("--cull-users")
            jupyterhub_idle_culler_role["scopes"].append("admin:users")

        if not get_config("cull.adminUsers"):
            cull_cmd.append("--cull-admin-users=false")

        if get_config("cull.removeNamedServers"):
            cull_cmd.append("--remove-named-servers")

        cull_max_age = get_config("cull.maxAge")
        if cull_max_age:
            cull_cmd.append(f"--max-age={cull_max_age}")

        c.JupyterHub.services.append(
            {
                "name": "jupyterhub-idle-culler",
                "command": cull_cmd,
            }
        )
        c.JupyterHub.load_roles.append(jupyterhub_idle_culler_role)

    for key, service in get_config("hub.services", {}).items():
        # c.JupyterHub.services is a list of dicts, but
        # hub.services is a dict of dicts to make the config mergable
        service.setdefault("name", key)

        # As the api_token could be exposed in hub.existingSecret, we need to read
        # it it from there or fall back to the chart managed k8s Secret's value.
        service.pop("apiToken", None)
        service["api_token"] = get_secret_value(f"hub.services.{key}.apiToken")

        c.JupyterHub.services.append(service)

    for key, role in get_config("hub.loadRoles", {}).items():
        # c.JupyterHub.load_roles is a list of dicts, but
        # hub.loadRoles is a dict of dicts to make the config mergable
        role.setdefault("name", key)

        c.JupyterHub.load_roles.append(role)

    # respect explicit null command (distinct from unspecified)
    # this avoids relying on KubeSpawner.cmd's default being None
    _unspecified = object()
    specified_cmd = get_config("singleuser.cmd", _unspecified)
    if specified_cmd is not _unspecified:
        c.Spawner.cmd = specified_cmd

    set_config_if_not_none(c.Spawner, "default_url", "singleuser.defaultUrl")

    cloud_metadata = get_config("singleuser.cloudMetadata")

    if cloud_metadata.get("blockWithIptables") == True:
        # Use iptables to block access to cloud metadata by default
        network_tools_image_name = get_config("singleuser.networkTools.image.name")
        network_tools_image_tag = get_config("singleuser.networkTools.image.tag")
        network_tools_resources = get_config("singleuser.networkTools.resources")
        ip = cloud_metadata["ip"]
        ip_block_container = client.V1Container(
            name="block-cloud-metadata",
            image=f"{network_tools_image_name}:{network_tools_image_tag}",
            command=[
                "iptables",
                "--append",
                "OUTPUT",
                "--protocol",
                "tcp",
                "--destination",
                ip,
                "--destination-port",
                "80",
                "--jump",
                "DROP",
            ],
            security_context=client.V1SecurityContext(
                privileged=True,
                run_as_user=0,
                capabilities=client.V1Capabilities(add=["NET_ADMIN"]),
            ),
            resources=network_tools_resources,
        )

        c.KubeSpawner.init_containers.append(ip_block_container)


    if get_config("debug.enabled", False):
        c.JupyterHub.log_level = "DEBUG"
        c.Spawner.debug = True

    # load potentially seeded secrets
    #
    # NOTE: ConfigurableHTTPProxy.auth_token is set through an environment variable
    #       that is set using the chart managed secret.
    c.JupyterHub.cookie_secret = get_secret_value("hub.config.JupyterHub.cookie_secret")
    # NOTE: CryptKeeper.keys should be a list of strings, but we have encoded as a
    #       single string joined with ; in the k8s Secret.
    #
    c.CryptKeeper.keys = get_secret_value("hub.config.CryptKeeper.keys").split(";")

    # load hub.config values, except potentially seeded secrets already loaded
    for app, cfg in get_config("hub.config", {}).items():
        if app == "JupyterHub":
            cfg.pop("proxy_auth_token", None)
            cfg.pop("cookie_secret", None)
            cfg.pop("services", None)
        elif app == "ConfigurableHTTPProxy":
            cfg.pop("auth_token", None)
        elif app == "CryptKeeper":
            cfg.pop("keys", None)
        c[app].update(cfg)

    # load /usr/local/etc/jupyterhub/jupyterhub_config.d config files
    config_dir = "/usr/local/etc/jupyterhub/jupyterhub_config.d"
    if os.path.isdir(config_dir):
        for file_path in sorted(glob.glob(f"{config_dir}/*.py")):
            file_name = os.path.basename(file_path)
            print(f"Loading {config_dir} config: {file_name}")
            with open(file_path) as f:
                file_content = f.read()
            # compiling makes debugging easier: https://stackoverflow.com/a/437857
            exec(compile(source=file_content, filename=file_name, mode="exec"))

    # execute hub.extraConfig entries
    for key, config_py in sorted(get_config("hub.extraConfig", {}).items()):
        print(f"Loading extra config: {key}")
        exec(config_py)
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-jupyterhub-hub
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
data:
  ## Taken from upstream JupyterHub chart
  ## Source: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/tree/4.1.0/jupyterhub/files/hub
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.

    Methods here can be imported by extraConfig in values.yaml
    """
    import os
    from collections.abc import Mapping
    from functools import lru_cache

    import yaml


    # memoize so we only load config once
    @lru_cache
    def _load_config():
        """Load the Helm chart configuration used to render the Helm templates of
        the chart from a mounted k8s Secret, and merge in values from an optionally
        mounted secret (hub.existingSecret)."""

        cfg = {}
        for source in ("secret/values.yaml", "existing-secret/values.yaml"):
            path = f"/usr/local/etc/jupyterhub/{source}"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg


    @lru_cache
    def _get_config_value(key):
        """Load value from the k8s ConfigMap given a key."""

        path = f"/usr/local/etc/jupyterhub/config/{key}"
        if os.path.exists(path):
            with open(path) as f:
                return f.read()
        else:
            raise Exception(f"{path} not found!")


    @lru_cache
    def get_secret_value(key, default="never-explicitly-set"):
        """Load value from the user managed k8s Secret or the default k8s Secret
        given a key."""

        for source in ("existing-secret", "secret"):
            path = f"/usr/local/etc/jupyterhub/{source}/{key}"
            if os.path.exists(path):
                with open(path) as f:
                    return f.read()
        if default != "never-explicitly-set":
            return default
        raise Exception(f"{key} not found in either k8s Secret!")


    def get_name(name):
        """Returns the fullname of a resource given its short name"""
        return _get_config_value(name)


    def get_name_env(name, suffix=""):
        """Returns the fullname of a resource given its short name along with a
        suffix, converted to uppercase with dashes replaced with underscores. This
        is useful to reference named services associated environment variables, such
        as PROXY_PUBLIC_SERVICE_PORT."""
        env_key = _get_config_value(name) + suffix
        env_key = env_key.upper().replace("-", "_")
        return os.environ[env_key]


    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.

        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged


    def get_config(key, default=None):
        """
        Find a config item of a given name & return it

        Parses everything as YAML, so lists and dicts are available too

        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split("."):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value


    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
---
kind: Secret
apiVersion: v1
metadata:
  name: nova-jupyterhub-hub-tmpl
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
type: Opaque
stringData:
  values.ctmpl: |
    Chart:
      Name: jupyterhub
      Version: 9.0.11
    Release:
      Name: nova-jupyterhub
      Namespace: nova-jupyterhub
      Service: nova-jupyterhub
    hub:
      cookieSecret:
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      activeServerLimit:
      db:
        type: postgres
        url: postgresql://nova-ml-postgresql.nova-postgresql.svc:5432/jupyterhub
      services: {}
      allowNamedServers: false
      namedServerLimitPerUser:
      redirectToServer:
      shutdownOnLogout: true
    singleuser:
      podNameTemplate: jh-singleuser-{username}
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      tolerations: []
      networkTools:
        image:
          name: ${imageRepository}/bitnami/os-shell
          tag: 12-debian-12-r43
          digest:
          pullPolicy: IfNotPresent
          pullSecrets:
      cloudMetadata:
        blockWithIptables: false
      events: true
      extraLabels:
        app.kubernetes.io/name: nova-jupyterhub
        app.kubernetes.io/instance: nova-jupyterhub-singleuser
        app.kubernetes.io/version: 9.0.11
        app.kubernetes.io/managed-by: Nova
        hub.jupyter.org/network-access-hub: "true"
        app.kubernetes.io/component: singleuser
      uid: 1001
      fsGid: 1001
      containerSecurityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      podSecurityContext:
        enabled: true
        fsGroupChangePolicy: Always
        sysctls: []
        supplementalGroups: []
        fsGroup: 1001
      serviceAccountName: nova-jupyterhub-singleuser
      automountServiceAccountToken: false
      storage:
        type: dynamic
        extraLabels:
          app.kubernetes.io/name: nova-jupyterhub
          app.kubernetes.io/instance: nova-jupyterhub-singleuser
          app.kubernetes.io/managed-by: Nova
          app.kubernetes.io/component: singleuser
        extraVolumes:
          - name: empty-dir
            emptyDir:
              sizeLimit: 100Mi
              medium: Memory
          - name: trusted-ca-bundle
            configMap:
              name: trusted-ca-bundle
              items:
              - key: ca-certificates.pem
                path: ca-certificates.crt
        extraVolumeMounts:
          - name: empty-dir
            mountPath: /tmp
            subPath: tmp-dir
          - mountPath: /etc/ssl/certs
            name: trusted-ca-bundle
        capacity: 10Gi
        homeMountPath: /opt/bitnami/jupyterhub-singleuser
        dynamic:
          storageClass: ${jupyterHubStorageClass}
          pvcNameTemplate: jh-singleuser-claim-{username}{servername}
          volumeNameTemplate: jh-singleuser-volume-{username}{servername}
          storageAccessModes:
            - ReadWriteOnce
      image:
        name: ${imageRepository}/jupyter/git-base-notebook
        tag: 5.3.0-nova
        digest:
        pullPolicy: IfNotPresent
        pullSecrets:
      startTimeout: 300
      cpu:
        limit: 0.75
        guarantee: 0.5
      memory:
        limit: 768M
        guarantee: 512M
      cmd: jupyterhub-singleuser
      defaultUrl:
      profileList:
        - display_name: "Default Profile"
          description: "Default profile with 10Gi volume"
          default: true
        - display_name: "Minimal Profile"
          description: "Minimal profile with 5Gi volume"
          kubespawner_override: 
            storage_capacity: 5Gi
        - display_name: "Large Profile"
          description: "Large profile with 15Gi volume"
          kubespawner_override: 
            storage_capacity: 15Gi
        - display_name: "Extra Large Profile"
          description: "Extra large profile with 25Gi volume"
          kubespawner_override: 
            storage_capacity: 25Gi
    cull:
      enabled: true
      users: false
      removeNamedServers: false
      timeout: 3600
      every: 600
      concurrency: 10
      maxAge: 0
  hub.config.CryptKeeper.keys.ctmpl: '{{- with secret "nova-secrets/data/credentials/ml/jupyterhub" }}{{ .Data.data.cryptkeeper_key }}{{ end -}}'
  hub.config.JupyterHub.cookie_secret.ctmpl: '{{- with secret "nova-secrets/data/credentials/ml/jupyterhub" }}{{ .Data.data.jupyterhub_cookie_secret }}{{ end -}}'
---
kind: Secret
apiVersion: v1
metadata:
  name: nova-jupyterhub-hub-proxy-token
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub-proxy-token
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
type: Opaque
stringData:
  proxy-token: vault:nova-secrets/data/credentials/ml/jupyterhub#proxy_token
---
kind: Secret
apiVersion: v1
metadata:
  name: nova-jupyterhub-hub-db-secret
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub-proxy-token
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
type: Opaque
stringData:
  PGUSER: vault:nova-secrets/data/credentials/ml/postgresql#ml_username
  PGPASSWORD: vault:nova-secrets/data/credentials/ml/postgresql#ml_password
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nova-jupyterhub-hub
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/component: hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
  annotations:
    secret.reloader.stakater.com/reload: "nova-jupyterhub-hub-tmpl,nova-jupyterhub-hub-proxy-token,nova-jupyterhub-hub-db-secret"
    configmap.reloader.stakater.com/reload: "jupyterhub-vault-agent-config,nova-jupyterhub-hub-tmpl,nova-jupyterhub-hub,trusted-ca-bundle"
spec:
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: nova-jupyterhub-hub
      app.kubernetes.io/component: hub
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: nova-jupyterhub-hub
        app.kubernetes.io/component: hub
        hub.jupyter.org/network-access-proxy-api: "true"
        hub.jupyter.org/network-access-proxy-http: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        kubectl.kubernetes.io/default-container: hub
        vault.security.banzaicloud.io/vault-path: nova-kubernetes
        vault.security.banzaicloud.io/vault-role: nova-system-jupyterhub
        vault.security.banzaicloud.io/vault-trust-manager-tls-bundle: "trusted-ca-bundle"
        vault.security.banzaicloud.io/run-as-non-root: "true"
    spec:
      serviceAccountName: nova-jupyterhub-hub
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      tolerations: []
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: nova-jupyterhub-hub
      priorityClassName: system-cluster-critical
      initContainers:
        - name: secrets-webhook-mutation-helper
          image: ${imageRepository}/nova/secrets-webhook-mutation-helper:v1.0.0
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
          env:
            - name: CA_CERT_PATH
              value: /etc/ssl/certs/ca-certificates.crt
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - mountPath: /etc/ssl/certs
              name: trusted-ca-bundle
              readOnly: true
      containers:
        - name: vault-agent
          image: ${imageRepository}/starvault/starvault:v1.0.4
          env:
            - name: STARVAULT_ADDR
              value: https://nova-oauth.${dnsBaseDomain}
            - name: STARVAULT_CACERT
              value: /etc/ssl/certs/ca-certificates.crt
            - name: SKIP_SETCAP
              value: "true"
          volumeMounts:
            - name: vault-config
              mountPath: /etc/vault/config.hcl
              subPath: vault-agent-config.hcl
            - name: vault-secrets
              mountPath: /vault/secrets
            - mountPath: /etc/ssl/certs
              name: trusted-ca-bundle
              readOnly: true
            - mountPath: /var/run/secrets/vaultproject.io
              name: vault-agent-volume
            - name: tmpl-config
              mountPath: /tmp/agent/jupyterhub/jupyterhub_config.ctmpl
              subPath: jupyterhub_config.ctmpl
            - name: tmpl-secret
              mountPath: /tmp/agent/jupyterhub/values.ctmpl
              subPath: values.ctmpl
            - name: tmpl-secret
              mountPath: /tmp/agent/jupyterhub/hub.config.CryptKeeper.keys.ctmpl
              subPath: hub.config.CryptKeeper.keys.ctmpl
            - name: tmpl-secret
              mountPath: /tmp/agent/jupyterhub/hub.config.JupyterHub.cookie_secret.ctmpl
              subPath: hub.config.JupyterHub.cookie_secret.ctmpl
            - name: config-volume
              mountPath: /etc/jupyterhub
            - name: secret-volume
              mountPath: /usr/local/etc/jupyterhub/secret
          args:
            - agent
            - -config=/etc/vault/config.hcl
            - -log-level=debug
          securityContext:
            privileged: false
          ports:
            - containerPort: 8200
              name: vault
              protocol: TCP
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
        - name: hub
          image: ${imageRepository}/bitnami/jupyterhub:5.3.0-debian-12-r5
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - jupyterhub
          args:
            - --config
            - /etc/jupyterhub/jupyterhub_config.py
            - --upgrade-db
          ports:
            - name: http
              containerPort: 8081
          envFrom:
            - secretRef:
                name: nova-jupyterhub-hub-db-secret
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: HELM_RELEASE_NAME
              value: nova-jupyterhub
            - name: PROXY_API_SERVICE_PORT
              value: "8001"
            - name: HUB_SERVICE_PORT
              value: "8081"
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: nova-jupyterhub-hub-proxy-token
                  key: proxy-token
          resources:
            requests:
              cpu: 500m
              ephemeral-storage: 2Gi
              memory: 768Mi
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /hub/health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          livenessProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 3
          readinessProbe:
            failureThreshold: 30
            httpGet:
              path: /hub/health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: config-volume
              mountPath: /etc/jupyterhub
            - name: secret-volume
              mountPath: /usr/local/etc/jupyterhub/secret
            - mountPath: /etc/jupyterhub/z2jh.py
              subPath: z2jh.py
              name: config
            - mountPath: /etc/ssl/certs
              name: trusted-ca-bundle
      volumes:
        - name: empty-dir
          emptyDir:
            sizeLimit: 100Mi
            medium: Memory
        - name: vault-secrets
          emptyDir:
            sizeLimit: 1Mi
            medium: Memory
        - name: vault-config
          configMap:
            name: jupyterhub-vault-agent-config
        - emptyDir:
            sizeLimit: 1Mi
            medium: Memory
          name: vault-agent-volume
        - name: tmpl-config
          configMap:
            name: nova-jupyterhub-hub-tmpl
        - name: tmpl-secret
          secret:
            secretName: nova-jupyterhub-hub-tmpl
        - name: config-volume
          emptyDir:
            sizeLimit: 10Mi
            medium: Memory
        - name: secret-volume
          emptyDir:
            sizeLimit: 10Mi
            medium: Memory
        - name: config
          configMap:
            name: nova-jupyterhub-hub
        - name: trusted-ca-bundle
          configMap:
            name: trusted-ca-bundle
            items:
              - key: ca-certificates.pem
                path: ca-certificates.crt
---
apiVersion: v1
kind: Service
metadata:
  name: nova-jupyterhub-hub
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
    app.kubernetes.io/component: hub
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/instance: nova-jupyterhub-hub
    app.kubernetes.io/component: hub
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nova-jupyterhub-proxy
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/component: proxy
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
  annotations:
    secret.reloader.stakater.com/reload: "nova-jupyterhub-hub-proxy-token"
    configmap.reloader.stakater.com/reload: "trusted-ca-bundle"
spec:
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: nova-jupyterhub-proxy
      app.kubernetes.io/component: proxy
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: nova-jupyterhub-proxy
        app.kubernetes.io/component: proxy
        hub.jupyter.org/network-access-hub: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        kubectl.kubernetes.io/default-container: proxy
        vault.security.banzaicloud.io/vault-path: nova-kubernetes
        vault.security.banzaicloud.io/vault-role: nova-system-jupyterhub
        vault.security.banzaicloud.io/vault-trust-manager-tls-bundle: "trusted-ca-bundle"
        vault.security.banzaicloud.io/run-as-non-root: "true"
    spec:
      serviceAccountName: nova-jupyterhub-proxy
      terminationGracePeriodSeconds: 30
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      tolerations: []
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: nova-jupyterhub-proxy
      priorityClassName: system-cluster-critical
      securityContext:
        fsGroupChangePolicy: Always
        sysctls: []
        supplementalGroups: []
        fsGroup: 1001
      initContainers:
        - name: secrets-webhook-mutation-helper
          image: ${imageRepository}/nova/secrets-webhook-mutation-helper:v1.0.0
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
          env:
            - name: CA_CERT_PATH
              value: /etc/ssl/certs/ca-certificates.crt
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - mountPath: /etc/ssl/certs
              name: trusted-ca-bundle
              readOnly: true
      containers:
        - name: proxy
          image: ${imageRepository}/bitnami/configurable-http-proxy:4.6.3-debian-12-r9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
            runAsNonRoot: true
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
            seccompProfile:
              type: "RuntimeDefault"
          args:
            - configurable-http-proxy
            - "--ip=::"
            - "--api-ip=::"
            - --api-port=8001
            - --default-target=http://nova-jupyterhub-hub:8081
            - --error-target=http://nova-jupyterhub-hub:8081/hub/error
            - --port=8000
            - "--metrics-ip=::"
            - --metrics-port=8002
            - --log-level=debug
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
            - name: api
              containerPort: 8001
              protocol: TCP
            - name: metrics
              containerPort: 8002
              protocol: TCP
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: nova-jupyterhub-hub-proxy-token
                  key: proxy-token
          resources:
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          livenessProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 3
          readinessProbe:
            failureThreshold: 30
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: empty-dir
          emptyDir:
            sizeLimit: 100Mi
            medium: Memory
        - name: trusted-ca-bundle
          configMap:
            name: trusted-ca-bundle
            items:
              - key: ca-certificates.pem
                path: ca-certificates.crt
---
apiVersion: v1
kind: Service
metadata:
  name: nova-jupyterhub-proxy-api
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
spec:
  ports:
    - name: http
      port: 8001
      targetPort: api
      protocol: TCP
  selector:
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/component: proxy
---
apiVersion: v1
kind: Service
metadata:
  name: nova-jupyterhub-proxy-public
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/component: proxy
---
apiVersion: v1
kind: Service
metadata:
  name: nova-jupyterhub-proxy-metrics
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
    app.kubernetes.io/component: proxy
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8002
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/instance: nova-jupyterhub-proxy
    app.kubernetes.io/component: proxy
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nova-jupyterhub-image-puller
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/instance: nova-jupyterhub-image-puller
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
    app.kubernetes.io/component: image-puller
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: nova-jupyterhub-image-puller
      app.kubernetes.io/component: image-puller
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: nova-jupyterhub-image-puller
        app.kubernetes.io/component: image-puller
    spec:
      serviceAccountName: nova-jupyterhub-image-puller
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      tolerations: []
      priorityClassName: system-node-critical
      securityContext:
        fsGroupChangePolicy: Always
        sysctls: []
        supplementalGroups: []
        fsGroup: 1001
      initContainers:
        - name: pull-0
          image: ${imageRepository}/jupyter/git-base-notebook:5.3.0-nova
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          resources:
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
        - name: pull-1
          image: ${imageRepository}/bitnami/os-shell:12-debian-12-r43
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          resources:
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
      containers:
        - name: pause
          image: ${imageRepository}/bitnami/os-shell:12-debian-12-r43
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/sh
            - -c
            - sleep infinity
          resources:
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: empty-dir
          emptyDir:
            sizeLimit: 100Mi
            medium: Memory
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nova-jupyterhub
  namespace: nova-jupyterhub
  labels:
    app.kubernetes.io/name: nova-jupyterhub
    app.kubernetes.io/version: 9.0.11
    app.kubernetes.io/managed-by: Nova
spec:
  ingressClassName: nginx-public
  rules:
    - host: nova-jupyterhub-main.${mlBaseDomain}
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: nova-jupyterhub-proxy-public
                port:
                  name: http
  tls:
    - hosts:
        - nova-jupyterhub-main.${mlBaseDomain}
      secretName: ml-ingress-certificate
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nova-jupyterhub-hub
  namespace: nova-monitoring
  labels:
    app.kubernetes.io/instance: nova-jupyterhub-hub
spec:
  jobLabel: jupyterhub-hub
  endpoints:
    - port: http
      path: /hub/metrics
      interval: 30s
      honorLabels: false
  namespaceSelector:
    matchNames:
      - nova-jupyterhub
  selector:
    matchLabels:
      app.kubernetes.io/component: hub
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nova-jupyterhub-proxy
  namespace: nova-monitoring
  labels:
    app.kubernetes.io/instance: nova-jupyterhub-proxy
spec:
  jobLabel: jupyterhub-proxy
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
      honorLabels: false
  namespaceSelector:
    matchNames:
      - nova-jupyterhub
  selector:
    matchLabels:
      app.kubernetes.io/component: proxy
---
apiVersion: console.nova-platform.io/v1
kind: ConsoleLink
metadata:
  name: jupyterrhub-console-link
spec:
  href: "https://nova-jupyterhub-main.${mlBaseDomain}"
  location: ApplicationMenu
  text: "Jupyterhub"
  applicationMenu:
    section: "ML"
    imageURL: "data:image/png;base64,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"
